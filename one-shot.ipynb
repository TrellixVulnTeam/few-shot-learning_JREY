{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning\n",
    "\n",
    "<!-- **Challenge:** [Omniglot](https://github.com/brendenlake/omniglot), the \"transpose\" of MNIST, with 1,623 character classes, each with 20 examples. Is it possible to build a few-shot classifier with a target of <35% error rate? -->\n",
    "\n",
    "## Exploring the Dataset\n",
    "\n",
    "**[Omniglot](https://github.com/brendenlake/omniglot)** is a collection of 1,623 hand drawn characters from 50 alphabets. Each of the 1,623 characters is drawn online via *Amazon's Mechanical Turk* by 20 different people at resolution of `105x105`. It is sometimes reffered to as the **\"Trasnpose of MNIST\"**, since it has 1,623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.\n",
    "\n",
    "Data Structure:\n",
    "```sh\n",
    "$ tree all_runs/ -L 2\n",
    " all_runs\n",
    "├── run01\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run02\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run03\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "│\n",
    "├── ...\n",
    "│\n",
    "├── run18\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run19\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "└── run20\n",
    "    ├── class_labels.txt\n",
    "    ├── test\n",
    "    └── training\n",
    "\n",
    "60 directories, 20 files\n",
    "```\n",
    "\n",
    "Omniglot contains 20 examples called \"runs\" each run has `class_labels.txt`, `test` and `training` folders. Let's see what's going on visually:\n",
    "\n",
    "```sh\n",
    "$ tree all_runs/run0\n",
    "all_runs/run01\n",
    "├── class_labels.txt\n",
    "├── test\n",
    "│   ├── item01.png\n",
    "│   ├── item02.png\n",
    "│   ├── item03.png\n",
    "│   ├── item04.png\n",
    "│   ├── ...\n",
    "│   ├── item17.png\n",
    "│   ├── item18.png\n",
    "│   ├── item19.png\n",
    "│   └── item20.png\n",
    "└── training\n",
    "    ├── class01.png\n",
    "    ├── class02.png\n",
    "    ├── class03.png\n",
    "    ├── ...\n",
    "    ├── class17.png\n",
    "    ├── class18.png\n",
    "    ├── class19.png\n",
    "    └── class20.png\n",
    "\n",
    "2 directories, 41 files\n",
    "```\n",
    "\n",
    "### N-way One-shot learning\n",
    "\n",
    "Given a tiny labelled training set $S$, which has $ N $ examples, each vectors of the same dimension with a distinct label $y$.\n",
    "\n",
    "$$ S = \\{(x_i, y_i), ..., (x_N, y_N)\\} $$\n",
    "\n",
    "We're also given $\\hat{x}$, the test example it has to classify. Since exactly one example in the training set has the right class, the aim is to correctly predict which $y \\in S$ is the same as $\\hat{x}$'s label, $\\hat{y}$.\n",
    "\n",
    "- Real world problems might not always have the constraint that exactly one image has the correct class.\n",
    "\n",
    "- It's easy to generalize to k-shot learning by having there be $k$ examples for each $y_i$ rather than just one.\n",
    "\n",
    "- When $N$ is higher, there are more possible classes that $\\hat{x}$ can belong to, so it's harder to predict the correct one.\n",
    "\n",
    "- Randomly guessing will average ${100\\over n}\\%$ accuracy.\n",
    "\n",
    "\n",
    "Let's visualze some examples in the Omniglot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data directory.\n",
    "# data_dir = omniglot.data_dir\n",
    "\n",
    "# # Directory containing compressed files.\n",
    "# comp_dir = omniglot.compressed_dir\n",
    "\n",
    "# # Extract `all_runs`.\n",
    "# all_runs_dir = omniglot.Data.extract(os.path.join(comp_dir, 'all_runs.tar.gz'))\n",
    "# image_bg_dir = omniglot.Data.extract(os.path.join(comp_dir, 'images_background.tar.gz'))\n",
    "\n",
    "# # Visualize 1st run.\n",
    "# # Emphasis 1st matching digits in \"class_labels.txt\".\n",
    "# omniglot.Visualize.runs(directory=os.path.join(all_runs_dir, 'run01'),\n",
    "#                         index=0,  # Index to emphasize on.\n",
    "#                         title='Visualize 1st matching digits')\n",
    "\n",
    "# # Visualize one random character from 20 different classes of Greek alphabet.\n",
    "# omniglot.Visualize.symbols(directory=os.path.join(image_bg_dir, 'Greek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For OS operations.\n",
    "import os\n",
    "\n",
    "# For Mathematical operations.\n",
    "import numpy as np\n",
    "\n",
    "# For High-level ML operations.\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper package for omniglot's dataset & model.\n",
    "import omniglot\n",
    "\n",
    "# Helper file for preparing input data.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating saved/images_background...\n",
      "\n",
      "Extracing datasets/compressed/images_background.tar.gz...\n",
      "Sucessfully extracted to datasets/extracted/images_background\n",
      "Loading images & targets\n",
      "ID  Alphabet                                     Status\n",
      "00. Alphabet of the Magi                          DONE\n",
      "01. Anglo-Saxon Futhorc                           DONE\n",
      "02. Arcadian                                      DONE\n",
      "03. Armenian                                      DONE\n",
      "04. Asomtavruli (Georgian)                        DONE\n",
      "05. Balinese                                      DONE\n",
      "06. Bengali                                       DONE\n",
      "07. Blackfoot (Canadian Aboriginal Syllabics)     DONE\n",
      "08. Braille                                       DONE\n",
      "09. Burmese (Myanmar)                             DONE\n",
      "10. Cyrillic                                      DONE\n",
      "11. Early Aramaic                                 DONE\n",
      "12. Futurama                                      DONE\n",
      "13. Grantha                                       DONE\n",
      "14. Greek                                         DONE\n",
      "15. Gujarati                                      DONE\n",
      "16. Hebrew                                        DONE\n",
      "17. Inuktitut (Canadian Aboriginal Syllabics)     DONE\n",
      "18. Japanese (hiragana)                           DONE\n",
      "19. Japanese (katakana)                           DONE\n",
      "20. Korean                                        DONE\n",
      "21. Latin                                         DONE\n",
      "22. Malay (Jawi - Arabic)                         DONE\n",
      "23. Mkhedruli (Georgian)                          DONE\n",
      "24. N Ko                                          DONE\n",
      "25. Ojibwe (Canadian Aboriginal Syllabics)        DONE\n",
      "26. Sanskrit                                      DONE\n",
      "27. Syriac (Estrangelo)                           DONE\n",
      "28. Tagalog                                       DONE\n",
      "29. Tifinagh                                      DONE\n",
      "Cached \"images\" to \"saved/images_background/images.npy\"\n",
      "Cached \"targets\" to \"saved/images_background/targets.npy\"\n",
      "\n",
      "Images = (964, 20, 105, 105)\tTargets = (964, 1)\n",
      "\n",
      "\n",
      "Training data: (964, 105, 105, 1)\n",
      "train_data = Dataset(mode='TRAIN', cache=True, cache_dir='saved/images_background')\n",
      "Creating saved/images_evaluation...\n",
      "\n",
      "Extracing datasets/compressed/images_evaluation.tar.gz...\n",
      "Sucessfully extracted to datasets/extracted/images_evaluation\n",
      "Loading images & targets\n",
      "ID  Alphabet                                     Status\n",
      "00. Angelic                                       DONE\n",
      "01. Atemayar Qelisayer                            DONE\n",
      "02. Atlantean                                     DONE\n",
      "03. Aurek-Besh                                    DONE\n",
      "04. Avesta                                        DONE\n",
      "05. Ge ez                                         DONE\n",
      "06. Glagolitic                                    DONE\n",
      "07. Gurmukhi                                      DONE\n",
      "08. Kannada                                       DONE\n",
      "09. Keble                                         DONE\n",
      "10. Malayalam                                     DONE\n",
      "11. Manipuri                                      DONE\n",
      "12. Mongolian                                     DONE\n",
      "13. Old Church Slavonic (Cyrillic)                DONE\n",
      "14. Oriya                                         DONE\n",
      "15. Sylheti                                       DONE\n",
      "16. Syriac (Serto)                                DONE\n",
      "17. Tengwar                                       DONE\n",
      "18. Tibetan                                       DONE\n",
      "19. ULOG                                          DONE\n",
      "Cached \"images\" to \"saved/images_evaluation/images.npy\"\n",
      "Cached \"targets\" to \"saved/images_evaluation/targets.npy\"\n",
      "\n",
      "Images = (659, 20, 105, 105)\tTargets = (659, 1)\n",
      "\n",
      "\n",
      "Validation data: (659, 105, 105, 1)\n",
      "valid_data = Dataset(mode='VALIDATE', cache=True, cache_dir='saved/images_evaluation')\n"
     ]
    }
   ],
   "source": [
    "# Path to compressed training & validation set.\n",
    "train_path = os.path.join(omniglot.compressed_dir, 'images_background.tar.gz')\n",
    "valid_path = os.path.join(omniglot.compressed_dir, 'images_evaluation.tar.gz')\n",
    "\n",
    "# To avoid writing `omniglot.Dataset`...\n",
    "Dataset = omniglot.Dataset\n",
    "\n",
    "# Create training data instance.\n",
    "train_data = Dataset(path=train_path, mode=Dataset.Mode.TRAIN)\n",
    "print(f'\\nTraining data: {train_data.shape}')\n",
    "print(f'train_data = {train_data}')\n",
    "\n",
    "# Create validation data instance.\n",
    "valid_data = Dataset(path=valid_path, mode=Dataset.Mode.VALIDATE)\n",
    "print(f'\\nValidation data: {valid_data.shape}')\n",
    "print(f'valid_data = {valid_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters.\n",
    "steps_per_epoch, epochs, batch_size = 1000, 20, 32\n",
    "train_size, valid_size = train_data.n_classes, valid_data.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 0.6927 - acc: 0.4640 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.6931 - acc: 0.5060 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Saving model!\n",
      "Saved model weights to saved/models/weights.h5.\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Siamese Network Model.\n",
    "model = omniglot.SiameseNetwork()\n",
    "\n",
    "# Compile using: Adam optimizer,\n",
    "#                binary cross entropy loss function,\n",
    "#                and monitor accuracy metrics.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=model.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    # Get Training image pairs & targets.\n",
    "    train_pairs, train_targets = train_data.get_batch(train_size)\n",
    "    valid_pairs, valid_targets = valid_data.get_batch(valid_size)\n",
    "\n",
    "    # Train the network.\n",
    "    model.fit(train_pairs, train_targets, epochs=epochs, batch_size=batch_size,\n",
    "              validation_data=(valid_pairs, valid_targets))\n",
    "\n",
    "    # Save model weights.\n",
    "    print(f'\\n\\n{\"-\" * 65}\\nSaving model...')\n",
    "    model.save_weights(filepath=model.model_weights,\n",
    "                       overwrite=True, save_format=None)\n",
    "    print(f'Saved model weights to \"{model.model_weights}\"!')\n",
    "    print(f'{\"-\" * 65}\\n')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # When training is unexpectedly stopped!\n",
    "    print(f'\\n\\n{\"-\" * 65}')\n",
    "    print('Training interrupted!', 'Saving model...', sep='\\n')\n",
    "    model.save_weights(filepath=model.model_weights,\n",
    "                       overwrite=True, save_format=None)\n",
    "    print(f'Saved model weights to \"{model.model_weights}\"!')\n",
    "    print(f'{\"-\" * 65}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Models\n",
    "\n",
    "### One-Shot Learning Baseline over Nearest Neighbor\n",
    "\n",
    "<!-- TODO: Finish up docs. -->\n",
    "\n",
    "The simplest way to perform classification is with **K-Nearest Neighbors**, but since there are only one example per class, we're only allowed 1 nearest neighbor lookup –which is really bad! K-Nearest Neigbors usually performs well with 5 neighbors or more *(but this also depends on dataset & it's sparsity)*.\n",
    "\n",
    "Nearest Neighbor: This is just a way of measuring distance in a higher dimensional plane using distance metrics such as [Euclidean Distance]().\n",
    "\n",
    "$$ \\textrm{Euclidean Distance} = \\sqrt{\\sum_i^n{(q_i - p_i)^2}}$$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n",
    "\n",
    "$$ C(\\hat{x}) = \\underset{c \\in S}{\\operatorname{argmax}} \\big\\|\\hat{x} - x_c\\big\\| $$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
