{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning\n",
    "\n",
    "<!-- **Challenge:** [Omniglot](https://github.com/brendenlake/omniglot), the \"transpose\" of MNIST, with 1,623 character classes, each with 20 examples. Is it possible to build a few-shot classifier with a target of <35% error rate? -->\n",
    "\n",
    "Humans exhibit a strong ability to acquire and recognize new patterns. In particular, we observe that when presented with stimuli, people seem to be able to understand new concepts quickly and then recognize variations on these concepts in future percepts. [Machine learning]() has been successfully used to achieve *state-of- the-art* performance in a variety of applications such as web search, spam detection, caption generation, and speech and image recognition. However, these algorithms often break down when forced to make predictions about data for which little supervised information is available. We desire to generalize to these unfamiliar categories without necessitating extensive retraining which may be either expensive or impossible due to limited data or in an online prediction setting, such as web retrieval.\n",
    "\n",
    "One particularly interesting task is classification under the restriction that we may only observe a single example of each possible class before making a prediction about a test instance. This is called **[One-Shot Learning]()** and it is the primary focus in this notebook. This should be distinguished from **[Zero-shot Learning]()**, in which the model cannot look at any examples from the target classes.\n",
    "**[Few-Shot Learning]()** on the other hand; is a special case of one-shot learning that tries to learn from *a very small set of training examples* rather than one –as in the one-shot learning case.\n",
    "\n",
    "## Exploring the Dataset\n",
    "\n",
    "**[Omniglot](https://github.com/brendenlake/omniglot)** is a collection of **1,623** hand drawn characters from **50 alphabets**. Each of the 1,623 characters is drawn online via *Amazon's Mechanical Turk* by 20 different people at resolution of `105x105`. It is sometimes reffered to as the **\"Trasnpose of MNIST\"**, since it has 1,623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.\n",
    "\n",
    "### Structure of Omiglot Datasets\n",
    "\n",
    "```sh\n",
    "~ tree datasets/extracted -L 1\n",
    "datasets/extracted\n",
    "├── all_runs\n",
    "├── images_background\n",
    "└── images_evaluation\n",
    "\n",
    "3 directories, 0 files\n",
    "```\n",
    "\n",
    "Omniglot has 3 different dataset serving different purposes.\n",
    "\n",
    "- **`all_runs`**          - One-shot task.\n",
    "- **`images_background`** - Training set.\n",
    "- **`images_evaluation`** - Validation/ evaluation set.\n",
    "\n",
    "> **Usage:** The **`images_background`** will be used for training *a Model* for *Few-shot Learning* and then validated on **`images_evaluation`**. While the **`all_runs`** will be used for *One-shot task*.\n",
    "\n",
    "#### Training/Validation Dataset Structure\n",
    "\n",
    "The training data contains 30 different *Alphabets* and the validation data contains 20 different alphabets *(that aren't in the training set)*, each containing *Characters* written differently.\n",
    "\n",
    "```sh\n",
    "$datasets/extracted$ ~ tree images_background -L 2\n",
    "images_background\n",
    "├── Alphabet_of_the_Magi\n",
    "├── Anglo-Saxon_Futhorc\n",
    "├── Arcadian\n",
    "...\n",
    "├── Sanskrit\n",
    "├── Syriac_(Estrangelo)\n",
    "├── Tagalog\n",
    "└── Tifinagh\n",
    "\n",
    "30 directories, 0 files\n",
    "```\n",
    "\n",
    "#### Alphabet Structure\n",
    "\n",
    "A single *Alphabet* contains different *Characters*, each with 20 different *handwriting style*.\n",
    "\n",
    "```sh\n",
    "$datasets/extracted$ ~ tree images_background/Alphabet_of_the_Magi -L 2\n",
    "images_background/Alphabet_of_the_Magi\n",
    "├── character01\n",
    "│   ├── 0709_01.png\n",
    "│   ├── 0709_02.png\n",
    "│   ├── 0709_03.png\n",
    "│   ...\n",
    "│   ├── 0709_18.png\n",
    "│   ├── 0709_19.png\n",
    "│   └── 0709_20.png\n",
    "├── character02\n",
    "│   ├── 0710_01.png\n",
    "│   ├── 0710_02.png\n",
    "│   ├── 0710_03.png\n",
    "│   ...\n",
    "│   ├── 0710_18.png\n",
    "│   ├── 0710_19.png\n",
    "│   └── 0710_20.png\n",
    "...\n",
    "├── character19\n",
    "│   ├── 0727_01.png\n",
    "│   ├── 0727_02.png\n",
    "│   ├── 0727_03.png\n",
    "│   ...\n",
    "│   ├── 0727_18.png\n",
    "│   ├── 0727_19.png\n",
    "│   └── 0727_20.png\n",
    "└── character20\n",
    "    ├── 0728_01.png\n",
    "    ├── 0728_02.png\n",
    "    ├── 0728_03.png\n",
    "    ...\n",
    "    ├── 0728_18.png\n",
    "    ├── 0728_19.png\n",
    "    └── 0728_20.png\n",
    "\n",
    "20 directories, 400 files```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-way One-shot learning\n",
    "\n",
    "Given a tiny labelled training set $S$, which has $ N $ examples, each vectors of the same dimension with a distinct label $y$.\n",
    "\n",
    "$$ S = \\{(x_i, y_i), ..., (x_N, y_N)\\} $$\n",
    "\n",
    "We're also given $\\hat{x}$, the test example it has to classify. Since exactly one example in the training set has the right class, the aim is to correctly predict which $y \\in S$ is the same as $\\hat{x}$'s label, $\\hat{y}$.\n",
    "\n",
    "- Real world problems might not always have the constraint that exactly one image has the correct class.\n",
    "\n",
    "- It's easy to generalize to k-shot learning by having there be $k$ examples for each $y_i$ rather than just one.\n",
    "\n",
    "- When $N$ is higher, there are more possible classes that $\\hat{x}$ can belong to, so it's harder to predict the correct one.\n",
    "\n",
    "- Randomly guessing will average ${100\\over n}\\%$ accuracy.\n",
    "\n",
    "\n",
    "Let's visualze some examples in the Omniglot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data directory.\n",
    "# data_dir = omniglot.data_dir\n",
    "\n",
    "# # Directory containing compressed files.\n",
    "# comp_dir = omniglot.compressed_dir\n",
    "\n",
    "# # Extract `all_runs`.\n",
    "# all_runs_dir = omniglot.Data.extract(os.path.join(comp_dir, 'all_runs.tar.gz'))\n",
    "# image_bg_dir = omniglot.Data.extract(os.path.join(comp_dir, 'images_background.tar.gz'))\n",
    "\n",
    "# # Visualize 1st run.\n",
    "# # Emphasis 1st matching digits in \"class_labels.txt\".\n",
    "# omniglot.Visualize.runs(directory=os.path.join(all_runs_dir, 'run01'),\n",
    "#                         index=0,  # Index to emphasize on.\n",
    "#                         title='Visualize 1st matching digits')\n",
    "\n",
    "# # Visualize one random character from 20 different classes of Greek alphabet.\n",
    "# omniglot.Visualize.symbols(directory=os.path.join(image_bg_dir, 'Greek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For OS operations.\n",
    "import os\n",
    "\n",
    "# For Mathematical operations.\n",
    "import numpy as np\n",
    "\n",
    "# For High-level ML operations.\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper package for omniglot's dataset & model.\n",
    "import omniglot\n",
    "\n",
    "# Helper file for preparing input data.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating saved/images_background...\n",
      "\n",
      "Extracing datasets/compressed/images_background.tar.gz...\n",
      "Sucessfully extracted to datasets/extracted/images_background\n",
      "Loading images & targets\n",
      "ID  Alphabet                                     Status\n",
      "00. Alphabet of the Magi                          DONE\n",
      "01. Anglo-Saxon Futhorc                           DONE\n",
      "02. Arcadian                                      DONE\n",
      "03. Armenian                                      DONE\n",
      "04. Asomtavruli (Georgian)                        DONE\n",
      "05. Balinese                                      DONE\n",
      "06. Bengali                                       DONE\n",
      "07. Blackfoot (Canadian Aboriginal Syllabics)     DONE\n",
      "08. Braille                                       DONE\n",
      "09. Burmese (Myanmar)                             DONE\n",
      "10. Cyrillic                                      DONE\n",
      "11. Early Aramaic                                 DONE\n",
      "12. Futurama                                      DONE\n",
      "13. Grantha                                       DONE\n",
      "14. Greek                                         DONE\n",
      "15. Gujarati                                      DONE\n",
      "16. Hebrew                                        DONE\n",
      "17. Inuktitut (Canadian Aboriginal Syllabics)     DONE\n",
      "18. Japanese (hiragana)                           DONE\n",
      "19. Japanese (katakana)                           DONE\n",
      "20. Korean                                        DONE\n",
      "21. Latin                                         DONE\n",
      "22. Malay (Jawi - Arabic)                         DONE\n",
      "23. Mkhedruli (Georgian)                          DONE\n",
      "24. N Ko                                          DONE\n",
      "25. Ojibwe (Canadian Aboriginal Syllabics)        DONE\n",
      "26. Sanskrit                                      DONE\n",
      "27. Syriac (Estrangelo)                           DONE\n",
      "28. Tagalog                                       DONE\n",
      "29. Tifinagh                                      DONE\n",
      "Cached \"images\" to \"saved/images_background/images.npy\"\n",
      "Cached \"targets\" to \"saved/images_background/targets.npy\"\n",
      "\n",
      "Images = (964, 20, 105, 105)\tTargets = (964, 1)\n",
      "\n",
      "\n",
      "Training data: (964, 105, 105, 1)\n",
      "train_data = Dataset(mode='TRAIN', cache=True, cache_dir='saved/images_background')\n",
      "Creating saved/images_evaluation...\n",
      "\n",
      "Extracing datasets/compressed/images_evaluation.tar.gz...\n",
      "Sucessfully extracted to datasets/extracted/images_evaluation\n",
      "Loading images & targets\n",
      "ID  Alphabet                                     Status\n",
      "00. Angelic                                       DONE\n",
      "01. Atemayar Qelisayer                            DONE\n",
      "02. Atlantean                                     DONE\n",
      "03. Aurek-Besh                                    DONE\n",
      "04. Avesta                                        DONE\n",
      "05. Ge ez                                         DONE\n",
      "06. Glagolitic                                    DONE\n",
      "07. Gurmukhi                                      DONE\n",
      "08. Kannada                                       DONE\n",
      "09. Keble                                         DONE\n",
      "10. Malayalam                                     DONE\n",
      "11. Manipuri                                      DONE\n",
      "12. Mongolian                                     DONE\n",
      "13. Old Church Slavonic (Cyrillic)                DONE\n",
      "14. Oriya                                         DONE\n",
      "15. Sylheti                                       DONE\n",
      "16. Syriac (Serto)                                DONE\n",
      "17. Tengwar                                       DONE\n",
      "18. Tibetan                                       DONE\n",
      "19. ULOG                                          DONE\n",
      "Cached \"images\" to \"saved/images_evaluation/images.npy\"\n",
      "Cached \"targets\" to \"saved/images_evaluation/targets.npy\"\n",
      "\n",
      "Images = (659, 20, 105, 105)\tTargets = (659, 1)\n",
      "\n",
      "\n",
      "Validation data: (659, 105, 105, 1)\n",
      "valid_data = Dataset(mode='VALIDATE', cache=True, cache_dir='saved/images_evaluation')\n"
     ]
    }
   ],
   "source": [
    "# Path to compressed training & validation set.\n",
    "train_path = os.path.join(omniglot.compressed_dir, 'images_background.tar.gz')\n",
    "valid_path = os.path.join(omniglot.compressed_dir, 'images_evaluation.tar.gz')\n",
    "\n",
    "# To avoid writing `omniglot.Dataset`...\n",
    "Dataset = omniglot.Dataset\n",
    "\n",
    "# Create training data instance.\n",
    "train_data = Dataset(path=train_path, mode=Dataset.Mode.TRAIN)\n",
    "print(f'\\nTraining data: {train_data.shape}')\n",
    "print(f'train_data = {train_data}')\n",
    "\n",
    "# Create validation data instance.\n",
    "valid_data = Dataset(path=valid_path, mode=Dataset.Mode.VALIDATE)\n",
    "print(f'\\nValidation data: {valid_data.shape}')\n",
    "print(f'valid_data = {valid_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Models\n",
    "\n",
    "### One-Shot Learning Baseline over Nearest Neighbor\n",
    "\n",
    "<!-- TODO: Finish up docs. -->\n",
    "\n",
    "The simplest way to perform classification is with **K-Nearest Neighbors**, but since there are only one example per class, we're only allowed 1 nearest neighbor lookup –which is really bad! K-Nearest Neigbors usually performs well with 5 neighbors or more *(but this also depends on dataset & it's sparsity)*.\n",
    "\n",
    "Nearest Neighbor: This is just a way of measuring distance in a higher dimensional plane using distance metrics such as [Euclidean Distance]().\n",
    "\n",
    "$$ \\textrm{Euclidean Distance} = \\sqrt{\\sum_i^n{(q_i - p_i)^2}}$$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n",
    "\n",
    "$$ C(\\hat{x}) = \\underset{c \\in S}{\\operatorname{argmax}} \\big\\|\\hat{x} - x_c\\big\\| $$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters.\n",
    "steps_per_epoch, epochs, batch_size = 1000, 20, 32\n",
    "train_size, valid_size = train_data.n_classes, valid_data.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 0.6927 - acc: 0.4640 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 104s 208ms/step - loss: 0.6931 - acc: 0.5060 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Saving model!\n",
      "Saved model weights to saved/models/weights.h5.\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Siamese Network Model.\n",
    "model = omniglot.SiameseNetwork()\n",
    "\n",
    "# Compile using: Adam optimizer,\n",
    "#                binary cross entropy loss function,\n",
    "#                and monitor accuracy metrics.\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=model.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    # Get Training image pairs & targets.\n",
    "    train_pairs, train_targets = train_data.get_batch(train_size)\n",
    "    valid_pairs, valid_targets = valid_data.get_batch(valid_size)\n",
    "\n",
    "    # Train the network.\n",
    "    model.fit(train_pairs, train_targets, epochs=epochs, batch_size=batch_size,\n",
    "              validation_data=(valid_pairs, valid_targets))\n",
    "\n",
    "    # Save model weights.\n",
    "    print(f'\\n\\n{\"-\" * 65}\\nSaving model...')\n",
    "    model.save_weights(filepath=model.model_weights,\n",
    "                       overwrite=True, save_format=None)\n",
    "    print(f'Saved model weights to \"{model.model_weights}\"!')\n",
    "    print(f'{\"-\" * 65}\\n')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # When training is unexpectedly stopped!\n",
    "    print(f'\\n\\n{\"-\" * 65}')\n",
    "    print('Training interrupted!', 'Saving model...', sep='\\n')\n",
    "    model.save_weights(filepath=model.model_weights,\n",
    "                       overwrite=True, save_format=None)\n",
    "    print(f'Saved model weights to \"{model.model_weights}\"!')\n",
    "    print(f'{\"-\" * 65}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
