{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning\n",
    "\n",
    "**Challenge:** [Omniglot](https://github.com/brendenlake/omniglot), the \"transpose\" of MNIST, with 1,623 character classes, each with 20 examples. Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "## Exploring the Dataset\n",
    "\n",
    "**[Omniglot](https://github.com/brendenlake/omniglot)** is a collection of 1,623 hand drawn characters from 50 alphabets. Each of the 1,623 characters is drawn online via *Amazon's Mechanical Turk* by 20 different people at resolution of `105x105`. It is sometimes reffered to as the **\"Trasnpose of MNIST\"**, since it has 1,623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.\n",
    "\n",
    "Data Structure:\n",
    "```sh\n",
    "$ tree all_runs/ -L 2\n",
    " all_runs\n",
    "├── run01\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run02\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run03\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "│\n",
    "├── ...\n",
    "│\n",
    "├── run18\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run19\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "└── run20\n",
    "    ├── class_labels.txt\n",
    "    ├── test\n",
    "    └── training\n",
    "\n",
    "60 directories, 20 files\n",
    "```\n",
    "\n",
    "Omniglot contains 20 examples called \"runs\" each run has `class_labels.txt`, `test` and `training` folders. Let's see what's going on visually:\n",
    "\n",
    "```sh\n",
    "$ tree all_runs/run0\n",
    "all_runs/run01\n",
    "├── class_labels.txt\n",
    "├── test\n",
    "│   ├── item01.png\n",
    "│   ├── item02.png\n",
    "│   ├── item03.png\n",
    "│   ├── item04.png\n",
    "│   ├── ...\n",
    "│   ├── item17.png\n",
    "│   ├── item18.png\n",
    "│   ├── item19.png\n",
    "│   └── item20.png\n",
    "└── training\n",
    "    ├── class01.png\n",
    "    ├── class02.png\n",
    "    ├── class03.png\n",
    "    ├── ...\n",
    "    ├── class17.png\n",
    "    ├── class18.png\n",
    "    ├── class19.png\n",
    "    └── class20.png\n",
    "\n",
    "2 directories, 41 files\n",
    "```\n",
    "\n",
    "### N-way One-shot learning\n",
    "\n",
    "Given a tiny labelled training set $S$, which has $ N $ examples, each vectors of the same dimension with a distinct label $y$.\n",
    "\n",
    "$$ S = \\{(x_i, y_i), ..., (x_N, y_N)\\} $$\n",
    "\n",
    "We're also given $\\hat{x}$, the test example it has to classify. Since exactly one example in the training set has the right class, the aim is to correctly predict which $y \\in S$ is the same as $\\hat{x}$'s label, $\\hat{y}$.\n",
    "\n",
    "- Real world problems might not always have the constraint that exactly one image has the correct class.\n",
    "\n",
    "- It's easy to generalize to k-shot learning by having there be $k$ examples for each $y_i$ rather than just one.\n",
    "\n",
    "- When $N$ is higher, there are more possible classes that $\\hat{x}$ can belong to, so it's harder to predict the correct one.\n",
    "\n",
    "- Randomly guessing will average ${100\\over n}\\%$ accuracy.\n",
    "\n",
    "\n",
    "Let's visualze some examples in the Omniglot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data directory.\n",
    "# data_dir = omniglot.data_dir\n",
    "\n",
    "# # Directory containing compressed files.\n",
    "# comp_dir = omniglot.compressed_dir\n",
    "\n",
    "# # Extract `all_runs`.\n",
    "# all_runs_dir = omniglot.Data.extract(os.path.join(comp_dir, 'all_runs.tar.gz'))\n",
    "# image_bg_dir = omniglot.Data.extract(os.path.join(comp_dir, 'images_background.tar.gz'))\n",
    "\n",
    "# # Visualize 1st run.\n",
    "# # Emphasis 1st matching digits in \"class_labels.txt\".\n",
    "# omniglot.Visualize.runs(directory=os.path.join(all_runs_dir, 'run01'),\n",
    "#                         index=0,  # Index to emphasize on.\n",
    "#                         title='Visualize 1st matching digits')\n",
    "\n",
    "# # Visualize one random character from 20 different classes of Greek alphabet.\n",
    "# omniglot.Visualize.symbols(directory=os.path.join(image_bg_dir, 'Greek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For OS operations.\n",
    "import os\n",
    "\n",
    "# Dataset helper.\n",
    "import omniglot\n",
    "\n",
    "# Model helper.\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracing datasets/compressed/images_background.tar.gz...\n",
      "Sucessfully extracted to datasets/extracted/images_background\n",
      "Loading cached images & corresponding targets.\n",
      "Extracing datasets/compressed/images_evaluation.tar.gz...\n",
      "Sucessfully extracted to datasets/extracted/images_evaluation\n",
      "Loading cached images & corresponding targets.\n",
      "\n",
      "Training data: (964, 105, 105, 1)\n",
      "train_data = Dataset(mode='TRAIN', cache=True, cache_dir='saved/images_background/train')\n",
      "\n",
      "Validation data: (659, 105, 105, 1)\n",
      "val_data = Dataset(mode='VALIDATE', cache=True, cache_dir='saved/images_evaluation/validate')\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(omniglot.compressed_dir, 'images_background.tar.gz')\n",
    "val_path = os.path.join(omniglot.compressed_dir, 'images_evaluation.tar.gz')\n",
    "\n",
    "# To avoid writing `omniglot.Dataset`...\n",
    "Dataset = omniglot.Dataset\n",
    "\n",
    "# Create training & validation data instance.\n",
    "train_data = Dataset(path=train_path, mode=Dataset.Mode.TRAIN)\n",
    "val_data = Dataset(path=val_path, mode=Dataset.Mode.VALIDATE)\n",
    "\n",
    "print(f'\\nTraining data: {train_data.shape}')\n",
    "print(f'train_data = {train_data}')\n",
    "\n",
    "print(f'\\nValidation data: {val_data.shape}')\n",
    "print(f'val_data = {val_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese Network Model.\n",
    "network = model.SiameseNetwork()\n",
    "\n",
    "# Compile using AdamOptimizer & constractive loss function.\n",
    "network.compile(optimizer='adam',\n",
    "                loss=network.contrastive_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6b14156339ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/Documents/Code/IPython projects/few-shot-learning/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \"\"\"\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Sister networks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code/IPython projects/few-shot-learning/model.py\u001b[0m in \u001b[0;36m__encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Input layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;31m# x = self.input_layer(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Convolutional blocks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m           \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m           spec.max_ndim is not None):\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[1;32m   1381\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "# n_iter, batch_size = 1000, 64\n",
    "\n",
    "# for i in range(n_iter):\n",
    "#     pairs, target = train_data.get_batch(batch_size=batch_size)\n",
    "\n",
    "#     network.train_on_batch(x=pairs, y=target)\n",
    "\n",
    "#     break\n",
    "pairs, target = train_data.get_batch(1)\n",
    "\n",
    "network(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Models\n",
    "\n",
    "### One-Shot Learning Baseline over Nearest Neighbor\n",
    "\n",
    "The simplest way to perform classification is with **K-Nearest Neighbors**, but since there are only one example per class, we're only allowed 1 nearest neighbor lookup –which is really bad! K-Nearest Neigbors usually performs well with 5 neighbors or more *(but this also depends on dataset & it's sparsity)*.\n",
    "\n",
    "Nearest Neighbor: This is just a way of measuring distance in a higher dimensional plane using distance metrics such as [Euclidean Distance]().\n",
    "\n",
    "$$ \\textrm{Euclidean Distance} = \\sqrt{\\sum_i^n{(q_i - p_i)^2}}$$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n",
    "\n",
    "$$ C(\\hat{x}) = \\underset{c \\in S}{\\operatorname{argmax}} \\big\\|\\hat{x} - x_c\\big\\| $$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
