{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning\n",
    "\n",
    "**Challenge:** [Omniglot](https://github.com/brendenlake/omniglot), the \"transpose\" of MNIST, with 1,623 character classes, each with 20 examples. Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "## Exploring the Dataset\n",
    "\n",
    "**[Omniglot](https://github.com/brendenlake/omniglot)** is a collection of 1,623 hand drawn characters from 50 alphabets. Each of the 1,623 characters is drawn online via *Amazon's Mechanical Turk* by 20 different people at resolution of `105x105`. It is sometimes reffered to as the **\"Trasnpose of MNIST\"**, since it has 1,623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.\n",
    "\n",
    "Data Structure:\n",
    "```sh\n",
    "$ tree all_runs/ -L 2\n",
    " all_runs\n",
    "├── run01\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run02\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run03\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "│\n",
    "├── ...\n",
    "│\n",
    "├── run18\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run19\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "└── run20\n",
    "    ├── class_labels.txt\n",
    "    ├── test\n",
    "    └── training\n",
    "\n",
    "60 directories, 20 files\n",
    "```\n",
    "\n",
    "Omniglot contains 20 examples called \"runs\" each run has `class_labels.txt`, `test` and `training` folders. Let's see what's going on visually:\n",
    "\n",
    "```sh\n",
    "$ tree all_runs/run0\n",
    "all_runs/run01\n",
    "├── class_labels.txt\n",
    "├── test\n",
    "│   ├── item01.png\n",
    "│   ├── item02.png\n",
    "│   ├── item03.png\n",
    "│   ├── item04.png\n",
    "│   ├── ...\n",
    "│   ├── item17.png\n",
    "│   ├── item18.png\n",
    "│   ├── item19.png\n",
    "│   └── item20.png\n",
    "└── training\n",
    "    ├── class01.png\n",
    "    ├── class02.png\n",
    "    ├── class03.png\n",
    "    ├── ...\n",
    "    ├── class17.png\n",
    "    ├── class18.png\n",
    "    ├── class19.png\n",
    "    └── class20.png\n",
    "\n",
    "2 directories, 41 files\n",
    "```\n",
    "\n",
    "The Omniglot dataset contains 50 alphabets total. We generally split these into a background set of 30 alphabets and an evaluation set of 20 alphabets.\n",
    "\n",
    "Given a tiny labelled training set $S$, which has $ N $ examples, each vectors of the same dimension with a distinct label $y$.\n",
    "\n",
    "$$ S = \\{(x_i, y_i), ..., (x_N, y_N)\\} $$\n",
    "\n",
    "We're also given $\\hat{x}$, the test example it has to classify. Since exactly one example in the training set has the right class, the aim is to correctly predict which $y \\subset S$ is the same as $\\hat{x}$'s label, $\\hat{y}$.\n",
    "\n",
    "- Real world problems might not always have the constraint that exactly one image has the correct class.\n",
    "\n",
    "- It's easy to generalize to k-shot learning by having there be $k$ examples for each $y_i$ rather than just one.\n",
    "\n",
    "- When $N$ is higher, there are more possible classes that $\\hat{x}$ can belong to, so it's harder to predict the correct one.\n",
    "\n",
    "- Randomly guessing will average ${100\\over n}\\%$ accuracy.\n",
    "\n",
    "\n",
    "Let's visualze some examples in the Omniglot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run01', 'run02', 'run03', 'run04', 'run05', 'run06', 'run07', 'run08', 'run09', 'run10', 'run11', 'run12', 'run13', 'run14', 'run15', 'run16', 'run17', 'run18', 'run19', 'run20']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'all_runs'\n",
    "\n",
    "all_runs = os.listdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Models\n",
    "\n",
    "### One-Shot Learning Baseline over Nearest Neighbor\n",
    "\n",
    "The simplest way to perform classification is with **K-Nearest Neighbors**, but since there are only one example per class, we're only allowed 1 nearest neighbor lookup –which is really bad! K-Nearest Neigbors usually performs well with 5 neighbors or more *(but this also depends on dataset & it's sparsity)*.\n",
    "\n",
    "Nearest Neighbor: This is just a way of measuring distance in a higher dimensional plane using distance metrics such as [Euclidean Distance]().\n",
    "\n",
    "$$ \\textrm{Euclidean Distance} = \\sqrt{\\sum_i^n{(q_i - p_i)^2}}$$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n",
    "\n",
    "$$ C(\\hat{x}) = \\underset{c \\in S}{\\operatorname{argmax}} \\big\\|\\hat{x} - x_c\\big\\| $$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
