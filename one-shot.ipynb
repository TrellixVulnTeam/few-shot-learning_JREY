{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning\n",
    "\n",
    "**Challenge:** [Omniglot](https://github.com/brendenlake/omniglot), the \"transpose\" of MNIST, with 1,623 character classes, each with 20 examples. Build a few-shot classifier with a target of <35% error.\n",
    "\n",
    "## Exploring the Dataset\n",
    "\n",
    "**[Omniglot](https://github.com/brendenlake/omniglot)** is a collection of 1,623 hand drawn characters from 50 alphabets. Each of the 1,623 characters is drawn online via *Amazon's Mechanical Turk* by 20 different people at resolution of `105x105`. It is sometimes reffered to as the **\"Trasnpose of MNIST\"**, since it has 1,623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits.\n",
    "\n",
    "Data Structure:\n",
    "```sh\n",
    "$ tree all_runs/ -L 2\n",
    " all_runs\n",
    "├── run01\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run02\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run03\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "│\n",
    "├── ...\n",
    "│\n",
    "├── run18\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "├── run19\n",
    "│   ├── class_labels.txt\n",
    "│   ├── test\n",
    "│   └── training\n",
    "└── run20\n",
    "    ├── class_labels.txt\n",
    "    ├── test\n",
    "    └── training\n",
    "\n",
    "60 directories, 20 files\n",
    "```\n",
    "\n",
    "Omniglot contains 20 examples called \"runs\" each run has `class_labels.txt`, `test` and `training` folders. Let's see what's going on visually:\n",
    "\n",
    "```sh\n",
    "$ tree all_runs/run0\n",
    "all_runs/run01\n",
    "├── class_labels.txt\n",
    "├── test\n",
    "│   ├── item01.png\n",
    "│   ├── item02.png\n",
    "│   ├── item03.png\n",
    "│   ├── item04.png\n",
    "│   ├── ...\n",
    "│   ├── item17.png\n",
    "│   ├── item18.png\n",
    "│   ├── item19.png\n",
    "│   └── item20.png\n",
    "└── training\n",
    "    ├── class01.png\n",
    "    ├── class02.png\n",
    "    ├── class03.png\n",
    "    ├── ...\n",
    "    ├── class17.png\n",
    "    ├── class18.png\n",
    "    ├── class19.png\n",
    "    └── class20.png\n",
    "\n",
    "2 directories, 41 files\n",
    "```\n",
    "\n",
    "### N-way One-shot learning\n",
    "\n",
    "Given a tiny labelled training set $S$, which has $ N $ examples, each vectors of the same dimension with a distinct label $y$.\n",
    "\n",
    "$$ S = \\{(x_i, y_i), ..., (x_N, y_N)\\} $$\n",
    "\n",
    "We're also given $\\hat{x}$, the test example it has to classify. Since exactly one example in the training set has the right class, the aim is to correctly predict which $y \\in S$ is the same as $\\hat{x}$'s label, $\\hat{y}$.\n",
    "\n",
    "- Real world problems might not always have the constraint that exactly one image has the correct class.\n",
    "\n",
    "- It's easy to generalize to k-shot learning by having there be $k$ examples for each $y_i$ rather than just one.\n",
    "\n",
    "- When $N$ is higher, there are more possible classes that $\\hat{x}$ can belong to, so it's harder to predict the correct one.\n",
    "\n",
    "- Randomly guessing will average ${100\\over n}\\%$ accuracy.\n",
    "\n",
    "\n",
    "Let's visualze some examples in the Omniglot dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data directory.\n",
    "# data_dir = omniglot.data_dir\n",
    "\n",
    "# # Directory containing compressed files.\n",
    "# comp_dir = omniglot.compressed_dir\n",
    "\n",
    "# # Extract `all_runs`.\n",
    "# all_runs_dir = omniglot.Data.extract(os.path.join(comp_dir, 'all_runs.tar.gz'))\n",
    "# image_bg_dir = omniglot.Data.extract(os.path.join(comp_dir, 'images_background.tar.gz'))\n",
    "\n",
    "# # Visualize 1st run.\n",
    "# # Emphasis 1st matching digits in \"class_labels.txt\".\n",
    "# omniglot.Visualize.runs(directory=os.path.join(all_runs_dir, 'run01'),\n",
    "#                         index=0,  # Index to emphasize on.\n",
    "#                         title='Visualize 1st matching digits')\n",
    "\n",
    "# # Visualize one random character from 20 different classes of Greek alphabet.\n",
    "# omniglot.Visualize.symbols(directory=os.path.join(image_bg_dir, 'Greek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# For OS operations.\n",
    "import os\n",
    "\n",
    "# For Mathematical operations.\n",
    "import numpy as np\n",
    "\n",
    "# Dataset helper.\n",
    "import omniglot\n",
    "\n",
    "# Model helper.\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted to datasets/extracted/images_background\n",
      "Loading cached images & corresponding targets.\n",
      "Already extracted to datasets/extracted/images_evaluation\n",
      "Loading cached images & corresponding targets.\n",
      "\n",
      "Training data: (964, 105, 105, 1)\n",
      "train_data = Dataset(mode='TRAIN', cache=True, cache_dir='saved/images_background/train')\n",
      "\n",
      "Validation data: (659, 105, 105, 1)\n",
      "val_data = Dataset(mode='VALIDATE', cache=True, cache_dir='saved/images_evaluation/validate')\n"
     ]
    }
   ],
   "source": [
    "# Path to compressed training & validation set.\n",
    "train_path = os.path.join(omniglot.compressed_dir, 'images_background.tar.gz')\n",
    "val_path = os.path.join(omniglot.compressed_dir, 'images_evaluation.tar.gz')\n",
    "\n",
    "# To avoid writing `omniglot.Dataset`...\n",
    "Dataset = omniglot.Dataset\n",
    "\n",
    "# Create training data instance.\n",
    "train_data = Dataset(path=train_path, mode=Dataset.Mode.TRAIN)\n",
    "print(f'\\nTraining data: {train_data.shape}')\n",
    "print(f'train_data = {train_data}')\n",
    "\n",
    "# Create validation data instance.\n",
    "val_data = Dataset(path=val_path, mode=Dataset.Mode.VALIDATE)\n",
    "print(f'\\nValidation data: {val_data.shape}')\n",
    "print(f'val_data = {val_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters.\n",
    "steps_per_epoch, epochs = 1000, 10\n",
    "train_size, batch_size = train_data.n_classes, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "964/964 [==============================] - 197s 204ms/step - loss: 0.6998 - acc: 0.5093\n",
      "Epoch 2/10\n",
      "964/964 [==============================] - 192s 200ms/step - loss: 0.6930 - acc: 0.5176\n",
      "Epoch 3/10\n",
      " 64/964 [>.............................] - ETA: 2:53 - loss: 0.6931 - acc: 0.5000\n",
      "\n",
      "Training interrupted!\n"
     ]
    }
   ],
   "source": [
    "# Define the Siamese Network Model.\n",
    "network = model.SiameseNetwork()\n",
    "\n",
    "# Compile using: Adam optimizer,\n",
    "#                binary cross entropy loss function,\n",
    "#                and show accuracy metrics.\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss=network.binary_crossentropy,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    # Get Training image pairs & targets.\n",
    "    pairs, targets = train_data.get_batch(train_size)\n",
    "\n",
    "    # Train the network.\n",
    "    network.fit(pairs, targets, epochs=epochs,\n",
    "                batch_size=batch_size)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # When training is unexpectedly stopped!\n",
    "    print('\\n\\nTraining interrupted!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Models\n",
    "\n",
    "### One-Shot Learning Baseline over Nearest Neighbor\n",
    "\n",
    "The simplest way to perform classification is with **K-Nearest Neighbors**, but since there are only one example per class, we're only allowed 1 nearest neighbor lookup –which is really bad! K-Nearest Neigbors usually performs well with 5 neighbors or more *(but this also depends on dataset & it's sparsity)*.\n",
    "\n",
    "Nearest Neighbor: This is just a way of measuring distance in a higher dimensional plane using distance metrics such as [Euclidean Distance]().\n",
    "\n",
    "$$ \\textrm{Euclidean Distance} = \\sqrt{\\sum_i^n{(q_i - p_i)^2}}$$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n",
    "\n",
    "$$ C(\\hat{x}) = \\underset{c \\in S}{\\operatorname{argmax}} \\big\\|\\hat{x} - x_c\\big\\| $$\n",
    "\n",
    "After calculating the Euclidean disance over `k` nearest neighbors. We then take the closest one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
